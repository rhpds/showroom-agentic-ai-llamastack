= Web Search

One of the more powerful out of the box tools in Llama Stack is *web search* and an integration with Tavily.  

This means your agent/application can have the latest and greatest information available from Internet sources and not wholly rely on its training data.

This module requires you to have a *TAVILY_SEARCH_API_KEY*

Make sure you are in the correct base directory

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
cd $HOME/fantaco-redhat-one-2026/
pwd
----

[.console-output]
[source,bash,subs=attributes+]
----
/home/lab-user/fantaco-redhat-one-2026
----

If needed, create a Python virtual environment (venv) 

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
python -m venv .venv
----

How do you know if you need to create one? You can look for an existing .venv folder

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
ls .venv
----

The following response indicates you need to create your Python venv

[.console-output]
[source,bash,subs=attributes+]
----
ls: cannot access '.venv': No such file or directory
----

Set environment

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
source .venv/bin/activate 
----


Tavily was configured as part of the ConfigMap 

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
grep -A 3 "tavily" llama-stack-scripts/llamastack-configmap.yaml
----

[.console-output]
[source,bash,subs=attributes+]
----
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_SEARCH_API_KEY:=}
          max_results: 3   
----

Change to the web-search sub-directory

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
cd web-search
pwd
----


You will need an API key to activate.  Sign up at https://www.tavily.com/ and create an API key.


[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
export TAVILY_SEARCH_API_KEY=
----

Make sure your INFERENCE_MODEL env variable is set correctly

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
echo $INFERENCE_MODEL
----

[.console-output]
[source,bash,subs=attributes+]
----
vllm/qwen3-14b
----


[.console-output]
[source,bash,subs=attributes+]
----
/home/lab-user/fantaco-redhat-one-2026/web-search
----

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
pip install -r requirements.txt
----

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
python 1_list_tools.py
----

[.console-output]
[source,bash,subs=attributes+]
----
==================================================
Registered Toolgroups
==================================================

Toolgroup ID:  builtin::rag
Provider ID:   rag-runtime
--------------------------------------------------

Toolgroup ID:  builtin::websearch
Provider ID:   tavily-search
--------------------------------------------------

Total toolgroups: 2
==================================================
----

=== Who won the last Super Bowl?

Review the code snippet to use Response API to ask who won last Super Bowl.

[.console-input]
[source,python,role="copypaste",subs=attributes+]
----
# Initialize client
client = LlamaStackClient(base_url=LLAMA_STACK_BASE_URL)

# Create response without web search (non-streaming)
response = client.responses.create(
    model=INFERENCE_MODEL,
    input="Who won the last Super Bowl?",
    stream=False,
)
----

Run the script:

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
python 2_no_web_search.py
----

[.console-output]
[source,bash,subs=attributes+]
----
The last Super Bowl, **Super Bowl LVIII**, was played on **February 11, 2024**, at Allegiant Stadium in Las Vegas. The **Kansas City Chiefs** defeated the **San Francisco 49ers** with a final score of **25-22**.

**Patrick Mahomes** of the Chiefs was named **Super Bowl MVP**, leading his team to victory with a strong performance. This marked the Chiefs' second Super Bowl win in three years (following their 2020 victory).
----

It is very likely that the response you receive will be different than the one above. Typically the response includes the winner from 2023 or 2024.

Now we can use the builtin tool to use Tavily websearch to get the accurate up to date answer.

See the snippet below:

[.console-input]
[source,python,role="copypaste",subs=attributes+]
----
# Initialize client with Tavily API key
client = LlamaStackClient(
    base_url=LLAMA_STACK_BASE_URL,
    provider_data={"tavily_search_api_key": TAVILY_SEARCH_API_KEY} if TAVILY_SEARCH_API_KEY else None,
)

# Create response with web search (non-streaming)
response = client.responses.create(
    model=INFERENCE_MODEL,
    input=QUESTION,
    tools=[{"type": "web_search"}],
    stream=False,
)
----

Run the script:

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
QUESTION="Who won the last Super Bowl?" python 3_web_search.py
----

[.console-output]
[source,bash,subs=attributes+]
----
Tavily Key: Set
Question:   Who won the last Super Bowl?

The Philadelphia Eagles won the most recent Super Bowl (Super Bowl LIX in 2025), defeating the Kansas City Chiefs with a score of **40-22** in New Orleans. This marked the Eagles' second Super Bowl championship and their first since 2018.
----

If you are running this query after February 8 2026 then you should have a different response.

=== Who is the current US President?

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
python 2_no_web_search_president.py
----

[.console-output]
[source,bash,subs=attributes+]
----
As of the latest information available (up to July 2024), **Joe Biden** is the 46th President of the United States. His term began on **January 20, 2021**, following the 2020 presidential election. The next U.S. presidential election is scheduled for **November 5, 2024**, with the new president to be inaugurated on **January 20, 2025**.

If you're asking this question after July 2024, the answer may change depending on the election results. Let me know if you'd like updates on the 2024 race!
----

And now with a web search

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
python 3_web_search_president.py
----

[.console-output]
[source,bash,subs=attributes+]
----
The current President of the United States is **Donald John Trump**. 
He was sworn into office on **January 20, 2025**, and his term is set 
to end on **January 20, 2029**. This information is consistent across 
multiple sources, including official government websites and encyclopedic
references.

----

Note: Non-deterministic behavior means even with the web search tool 
this can report that Joe Biden is president.  


Make sure to know your model's knowledge cutoff date

=== Knowledge Cutoff Date

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
python 4_what_is_my_knowledge_cutoff.py
----

[.console-output]
[source,bash,subs=attributes+]
----
My knowledge cutoff is **Q3 2024**, meaning my training data includes 
information up to that time. However, I do **not** have real-time internet 
access, so I cannot provide updates or information beyond this cutoff. 
For the most current data, I recommend checking reliable sources directly. 
Let me know if you'd like help with anything within my training scope!
----

=== Today

A key tip to make your search results better is to simply add *Today* to every prompt.

----
today = date.today()
prompt=f"Today is {today}. Who is the current US President?"
----

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
python 5_web_search_president_today.py
----

[.console-output]
[source,bash,subs=attributes+]
----
As of January 8, 2026, the current U.S. President is **Donald Trump**. 
He secured a second term in the 2024 election, as indicated by multiple 
sources, including news articles and the official White House website.
----

=== Responses

You can also use the web search tool via the Responses API

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
QUESTION="Who won the last Super Bowl?"

cat << EOF | curl -sS "$LLAMA_STACK_BASE_URL/v1/responses" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $API_KEY" \
    -H "X-LlamaStack-Provider-Data: {\"tavily_search_api_key\": \"$TAVILY_SEARCH_API_KEY\"}" \
    -d @- | jq -r '.output[] | select(.type == "message") | .content[0].text'
{
  "model": "$INFERENCE_MODEL",
  "input": "$QUESTION",
  "tools": [{"type": "web_search"}],
  "stream": false
}
EOF
----




