= Exploring Llama Stack

Now that you have successfully deployed a Llama Stack instance, it's time to explore its capabilities through hands-on interaction with the APIs. In this module, you'll learn how to connect to your Llama Stack server, discover available models and APIs, and interact with the inference endpoints.

Llama Stack provides multiple API interfaces for different use cases:

* **Inference APIs**: For generating responses using language models
* **Agent APIs**: For building multi-turn conversational agents with tool calling
* **RAG APIs**: For retrieval-augmented generation with vector databases
* **Safety APIs**: For content moderation and guardrails
* **Tool APIs**: For integrating external capabilities

This exploration will help you understand the Llama Stack API landscape and prepare you for building agentic applications.

== Connecting to Llama Stack

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
export LLAMA_STACK_BASE_URL=http://llamastack-distribution-vllm-service:8321
export INFERENCE_MODEL=vllm/qwen3-14b-gaudi
----

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
echo "LLAMA_STACK_BASE_URL="$LLAMA_STACK_BASE_URL
echo "INFERENCE_MODEL="$INFERENCE_MODEL
----

[.console-output]
[source,bash,subs=attributes+]
----
LLAMA_STACK_BASE_URL=http://llamastack-distribution-vllm-service:8321
INFERENCE_MODEL=vllm/qwen3-14b-gaudi
----

The Llama Stack server is accessible via its internal Kubernetes service URL on port 8321. The `INFERENCE_MODEL` variable specifies which model to use for inference requests - in this lab, we're using the Qwen3-14B model optimized for Intel Gaudi accelerators.

== Discovering available resources

=== List available models

[source,bash,role="copypaste",subs=attributes+]
----
curl -sS $LLAMA_STACK_BASE_URL/v1/models \
     -H "Content-Type: application/json" \
     | jq -r '.data[].identifier'
----

[.console-output]
[source,bash,subs=attributes+]
----
granite-embedding-125m
sentence-transformers/nomic-ai/nomic-embed-text-v1.5
vllm/qwen3-14b-gaudi
vllm/Llama-Guard-3-1B
vllm/nomic-embed-text-v1-5
vllm/qwen25-7b-instruct
vllm/llama-scout-17b
----

Note: Your list of models may vary depending on what is available on the MaaS

Your Llama Stack distribution provides access to multiple models. The local vLLM-hosted model (`vllm/qwen3-14b-gaudi`) runs directly on your OpenShift cluster, while the Bedrock models can be accessed through AWS when configured. The embedding model enables vector search and RAG capabilities.

=== List APIs

Llama Stack exposes a comprehensive set of RESTful APIs following the OpenAPI specification. These APIs provide standardized interfaces for inference, agents, RAG, safety, and operational tasks. Let's explore the available endpoints:

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
curl -sS $LLAMA_STACK_BASE_URL/openapi.json \
     | jq '.paths | keys'
----

[.console-output]
[source,json]
----
[
  "/v1/agents",
  "/v1/agents/{agent_id}",
  "/v1/agents/{agent_id}/session",
  "/v1/agents/{agent_id}/session/{session_id}",
  "/v1/agents/{agent_id}/session/{session_id}/turn",
  "/v1/agents/{agent_id}/session/{session_id}/turn/{turn_id}",
  "/v1/agents/{agent_id}/session/{session_id}/turn/{turn_id}/resume",
  "/v1/agents/{agent_id}/session/{session_id}/turn/{turn_id}/step/{step_id}",
  "/v1/agents/{agent_id}/sessions",
  "/v1/batches",
  "/v1/batches/{batch_id}",
  "/v1/batches/{batch_id}/cancel",
  "/v1/chat/completions",
  "/v1/chat/completions/{completion_id}",
  "/v1/completions",
  "/v1/conversations",
  "/v1/conversations/{conversation_id}",
  "/v1/conversations/{conversation_id}/items",
  "/v1/conversations/{conversation_id}/items/{item_id}",
  "/v1/datasetio/append-rows/{dataset_id}",
  "/v1/datasetio/iterrows/{dataset_id}",
  "/v1/datasets",
  "/v1/datasets/{dataset_id}",
  "/v1/embeddings",
  "/v1/eval/benchmarks",
  "/v1/eval/benchmarks/{benchmark_id}",
  "/v1/eval/benchmarks/{benchmark_id}/evaluations",
  "/v1/eval/benchmarks/{benchmark_id}/jobs",
  "/v1/eval/benchmarks/{benchmark_id}/jobs/{job_id}",
  "/v1/eval/benchmarks/{benchmark_id}/jobs/{job_id}/result",
  "/v1/files",
  "/v1/files/{file_id}",
  "/v1/files/{file_id}/content",
  "/v1/health",
  "/v1/inspect/routes",
  "/v1/models",
  "/v1/models/{model_id}",
  "/v1/moderations",
  "/v1/openai/v1/batches",
  "/v1/openai/v1/batches/{batch_id}",
  "/v1/openai/v1/batches/{batch_id}/cancel",
  "/v1/openai/v1/chat/completions",
  "/v1/openai/v1/chat/completions/{completion_id}",
  "/v1/openai/v1/completions",
  "/v1/openai/v1/embeddings",
  "/v1/openai/v1/files",
  "/v1/openai/v1/files/{file_id}",
  "/v1/openai/v1/files/{file_id}/content",
  "/v1/openai/v1/models",
  "/v1/openai/v1/moderations",
  "/v1/openai/v1/responses",
  "/v1/openai/v1/responses/{response_id}",
  "/v1/openai/v1/responses/{response_id}/input_items",
  "/v1/openai/v1/vector_stores",
  "/v1/openai/v1/vector_stores/{vector_store_id}",
  "/v1/openai/v1/vector_stores/{vector_store_id}/file_batches",
  "/v1/openai/v1/vector_stores/{vector_store_id}/file_batches/{batch_id}",
  "/v1/openai/v1/vector_stores/{vector_store_id}/file_batches/{batch_id}/cancel",
  "/v1/openai/v1/vector_stores/{vector_store_id}/file_batches/{batch_id}/files",
  "/v1/openai/v1/vector_stores/{vector_store_id}/files",
  "/v1/openai/v1/vector_stores/{vector_store_id}/files/{file_id}",
  "/v1/openai/v1/vector_stores/{vector_store_id}/files/{file_id}/content",
  "/v1/openai/v1/vector_stores/{vector_store_id}/search",
  "/v1/post-training/job/artifacts",
  "/v1/post-training/job/cancel",
  "/v1/post-training/job/status",
  "/v1/post-training/jobs",
  "/v1/post-training/preference-optimize",
  "/v1/post-training/supervised-fine-tune",
  "/v1/prompts",
  "/v1/prompts/{prompt_id}",
  "/v1/prompts/{prompt_id}/set-default-version",
  "/v1/prompts/{prompt_id}/versions",
  "/v1/providers",
  "/v1/providers/{provider_id}",
  "/v1/responses",
  "/v1/responses/{response_id}",
  "/v1/responses/{response_id}/input_items",
  "/v1/safety/run-shield",
  "/v1/scoring-functions",
  "/v1/scoring-functions/{scoring_fn_id}",
  "/v1/scoring/score",
  "/v1/scoring/score-batch",
  "/v1/shields",
  "/v1/shields/{identifier}",
  "/v1/tool-runtime/invoke",
  "/v1/tool-runtime/list-tools",
  "/v1/tool-runtime/rag-tool/insert",
  "/v1/tool-runtime/rag-tool/query",
  "/v1/toolgroups",
  "/v1/toolgroups/{toolgroup_id}",
  "/v1/tools",
  "/v1/tools/{tool_name}",
  "/v1/vector-io/insert",
  "/v1/vector-io/query",
  "/v1/vector_stores",
  "/v1/vector_stores/{vector_store_id}",
  "/v1/vector_stores/{vector_store_id}/file_batches",
  "/v1/vector_stores/{vector_store_id}/file_batches/{batch_id}",
  "/v1/vector_stores/{vector_store_id}/file_batches/{batch_id}/cancel",
  "/v1/vector_stores/{vector_store_id}/file_batches/{batch_id}/files",
  "/v1/vector_stores/{vector_store_id}/files",
  "/v1/vector_stores/{vector_store_id}/files/{file_id}",
  "/v1/vector_stores/{vector_store_id}/files/{file_id}/content",
  "/v1/vector_stores/{vector_store_id}/search",
  "/v1/version",
  "/v1alpha/agents",
  "/v1alpha/agents/{agent_id}",
  "/v1alpha/agents/{agent_id}/session",
  "/v1alpha/agents/{agent_id}/session/{session_id}",
  "/v1alpha/agents/{agent_id}/session/{session_id}/turn",
  "/v1alpha/agents/{agent_id}/session/{session_id}/turn/{turn_id}",
  "/v1alpha/agents/{agent_id}/session/{session_id}/turn/{turn_id}/resume",
  "/v1alpha/agents/{agent_id}/session/{session_id}/turn/{turn_id}/step/{step_id}",
  "/v1alpha/agents/{agent_id}/sessions",
  "/v1alpha/eval/benchmarks",
  "/v1alpha/eval/benchmarks/{benchmark_id}",
  "/v1alpha/eval/benchmarks/{benchmark_id}/evaluations",
  "/v1alpha/eval/benchmarks/{benchmark_id}/jobs",
  "/v1alpha/eval/benchmarks/{benchmark_id}/jobs/{job_id}",
  "/v1alpha/eval/benchmarks/{benchmark_id}/jobs/{job_id}/result",
  "/v1alpha/inference/rerank",
  "/v1alpha/post-training/job/artifacts",
  "/v1alpha/post-training/job/cancel",
  "/v1alpha/post-training/job/status",
  "/v1alpha/post-training/jobs",
  "/v1alpha/post-training/preference-optimize",
  "/v1alpha/post-training/supervised-fine-tune",
  "/v1beta/datasetio/append-rows/{dataset_id}",
  "/v1beta/datasetio/iterrows/{dataset_id}",
  "/v1beta/datasets",
  "/v1beta/datasets/{dataset_id}"
]
----

This comprehensive API surface demonstrates Llama Stack's capabilities across the entire AI application lifecycle. Notice the variety of endpoints:

* **Inference endpoints** (`/v1/chat/completions`, `/v1/responses`, `/v1/completions`): Different approaches to getting model responses
* **Agent endpoints** (`/v1/agents/*`): Full agent lifecycle including sessions, turns, and steps
* **RAG and vector endpoints** (`/v1/vector_stores/*`, `/v1/vector-io/*`): Document storage and semantic search
* **Tool runtime** (`/v1/toolgroups`, `/v1/tools/*`): External tool integration
* **Safety and moderation** (`/v1/safety/*`, `/v1/shields/*`): Content filtering and guardrails
* **OpenAI compatibility layer** (`/v1/openai/v1/*`): Drop-in replacement for OpenAI API clients

== Inference APIs

Llama Stack provides multiple inference APIs to suit different application needs. Let's explore two primary approaches to getting responses from language models.

=== ChatCompletion API

The ChatCompletion API provides an OpenAI-compatible interface for conversational interactions. This API is familiar to developers who have worked with OpenAI's GPT models and supports streaming, function calling, and multi-turn conversations.

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
export QUESTION="what model are you?"
----

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
curl -sS $LLAMA_STACK_BASE_URL/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer fake" \
    -d "{
       \"model\": \"$INFERENCE_MODEL\",
       \"messages\": [{\"role\": \"user\", \"content\": \"$QUESTION\"}],
       \"temperature\": 0.0
     }" | jq -r '.choices[0].message.content'
----

[.console-output]
[source,bash,subs=attributes+]
----
I am Qwen, a large language model developed by Alibaba Cloud. I can answer questions, create text, and assist with various tasks. How can I help you today?
----

=== Response API

The **Response API** (`/v1/responses`) is a key differentiator of Llama Stack and represents a more streamlined, purpose-built interface for AI applications. Unlike the ChatCompletion API which focuses on conversational patterns, the Response API is designed for modern agentic workflows where you need direct, structured interaction with language models.

==== Why the Response API matters

The Response API is significant because it:

**Simplifies the interface**::
Instead of managing complex message arrays and conversation history, you provide direct input and receive structured output. This makes it ideal for single-turn interactions, function calling, and tool-augmented generation.

**Supports native Llama Stack features**::
The Response API is the native interface for Llama Stack's advanced capabilities including safety shields, structured outputs, and multi-step agent reasoning. It's designed specifically for the Llama Stack ecosystem rather than being a compatibility layer.

**Enables agentic patterns**::
Modern AI applications often need to break complex tasks into steps, invoke tools, and synthesize results. The Response API's design makes it natural to build these patterns, as you'll see in later modules when we build agents.

**Provides consistent semantics**::
The Response API maintains consistent behavior across different model providers (vLLM, Bedrock, Fireworks). This abstraction allows you to switch backends without changing application code.

.Response API workflow
[mermaid,width=100%]
----
sequenceDiagram
    participant App as Your Application
    participant LS as Llama Stack Server
    participant Model as vLLM Model
    participant Tools as External Tools

    App->>LS: POST /v1/responses<br/>{model, input}
    LS->>LS: Apply safety shields
    LS->>Model: Forward to inference provider
    Model->>Model: Generate response
    Model-->>LS: Raw model output
    LS->>LS: Structure output
    LS->>Tools: (Optional) Execute tools
    Tools-->>LS: Tool results
    LS-->>App: {output: [{content}]}

    Note over App,Tools: Response API handles orchestration,<br/>safety, and tool execution
----

==== Llama Stack's Response API support

Llama Stack provides first-class support for the Response API across its entire distribution ecosystem. Whether you're using vLLM for local inference, AWS Bedrock for managed models, or Fireworks AI for hosted endpoints, the Response API maintains consistent semantics and behavior.

Key features of Llama Stack's Response API implementation:

* **Provider abstraction**: Same API works with any configured inference provider
* **Automatic safety**: Integrated with Llama Stack's safety shields and content moderation
* **Structured outputs**: Native support for JSON schemas and structured data extraction
* **Tool integration**: Built-in support for RAG, web search, and custom tools
* **Observability**: Request/response logging and monitoring through OpenAPI endpoints

Let's interact with the Response API:

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
export QUESTION="What is the capital of Italy?"

curl -sS "$LLAMA_STACK_BASE_URL/v1/responses" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $API_KEY" \
    -d "{
      \"model\": \"$INFERENCE_MODEL\",
      \"input\": \"$QUESTION\"
    }" | jq -r '.output[0].content[0].text'
----

[.console-output]
[source,bash,subs=attributes+]
----
The capital of Italy is **Rome**. It is a historic city known for its rich cultural heritage, ancient landmarks like the Colosseum and Vatican City, and its role as the political and administrative center of the country.
----

The Response API provides a clean, direct interface for getting model responses. Notice the simplified request structure compared to ChatCompletion - you specify a `model` and `input` rather than managing message arrays. The response is structured as an array of output content, making it easy to extract text, handle tool calls, or process structured data.

This API becomes even more powerful when combined with Llama Stack's agent framework, which uses the Response API as the foundation for multi-step reasoning and tool execution.

== Tool runtime and capabilities

Beyond inference, Llama Stack provides a runtime environment for tools that agents can use to interact with external systems. These tools extend the capabilities of language models by allowing them to search the web, query databases, perform calculations, and integrate with enterprise systems.

=== Available tool groups

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
curl -sS -H "Content-Type: application/json" $LLAMA_STACK_BASE_URL/v1/toolgroups | jq
----

[.console-output]
[source,json,subs=attributes+]
----
{
  "data": [
    {
      "identifier": "builtin::rag",
      "provider_resource_id": "builtin::rag",<1>
      "provider_id": "rag-runtime",
      "type": "tool_group",
      "mcp_endpoint": null,
      "args": null
    }
  ]
}
----
<1> Use built-in RAG

Your Llama Stack distribution comes pre-configured with the built-in tool group:

* **RAG runtime**: Provides document retrieval and semantic search capabilities

These tools can be invoked by agents during their reasoning process, allowing them to augment their knowledge with external data. In later modules, you'll see how agents automatically decide when and how to use these tools.

=== Model Context Protocol (MCP)

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
curl -sS -H "Content-Type: application/json" \
  "$LLAMA_STACK_BASE_URL/v1/toolgroups" \
| jq -r '.data[] | select(.provider_id == "model-context-protocol") | .identifier'
----

There are no MCP Servers deployed and registered with this Llama Stack instance at this time.

The Model Context Protocol is an open standard for integrating external tools and data sources with language models. While this instance doesn't have MCP servers configured yet, you'll learn how to add MCP capabilities in the dedicated MCP module later in this lab.

== Summary

In this module, you explored the Llama Stack API landscape through hands-on interaction:

* **Connected to Llama Stack** and discovered available models across local and cloud providers
* **Explored the API surface** with over 100 endpoints for inference, agents, RAG, safety, and tools
* **Compared inference approaches** between ChatCompletion and Response APIs
* **Emphasized the Response API** as Llama Stack's native interface for agentic workflows, with provider abstraction, safety integration, and simplified semantics
* **Discovered tool capabilities** including web search and RAG runtime

The Response API's design philosophy - direct input/output, provider abstraction, and native support for Llama Stack features - makes it the foundation for building sophisticated AI agents. In the following modules, you'll use these APIs to implement RAG, integrate tools via MCP, and build complete agentic applications.

