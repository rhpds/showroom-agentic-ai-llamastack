= Deploying Llama Stack

== Understanding Llama Stack

Llama Stack is an open source standardized, opinionated framework for building and deploying generative AI applications. It provides a unified API interface and building blocks that abstract the complexity of working with large language models, enabling developers to focus on building agentic AI applications rather than managing infrastructure.

As part of Red Hat AI 3 running on this lab, Llama Stack brings enterprise-grade capabilities for deploying production AI workloads with the scalability, security, and operational excellence that Red Hat platforms provide.

=== What is Llama Stack?

Llama Stack is designed around the concept of distributions - pre-configured stacks that combine inference engines, model providers, and additional capabilities like retrieval-augmented generation (RAG), safety guardrails, and tool execution. Each distribution is a composition of:

* **Inference providers**: vLLM, AWS Bedrock, Fireworks AI, Together AI, and others
* **Memory and vector stores**: For conversational context and retrieval
* **Safety components**: Content moderation and guardrails
* **Tool runtime**: Integration with external systems and APIs
* **Agent framework**: Orchestration for multi-step reasoning

.Llama Stack core architecture
[mermaid,width=100%]
----
graph TB
    subgraph Application["Application Layer"]
        APP[Agentic AI Applications<br/>Chat Interfaces<br/>Custom Tools]
    end

    subgraph API["Llama Stack Unified API Layer"]
        INF[Inference API]
        AGT[Agents API]
        RAG[RAG API]
        SAF[Safety API]
        MEM[Memory API]
        TOL[Tools API]
    end

    subgraph Distribution["Distribution Layer"]
        PROV[Provider Adapters<br/>vLLM, Bedrock, Fireworks]
        VECT[Vector Stores<br/>Chroma, FAISS]
        SAFE[Safety Runtime<br/>Llama Guard]
        TOOL[Tool Runtime<br/>Built-in Tools]
    end

    subgraph Infra["Infrastructure Layer"]
        OCP[OpenShift Platform]
        VLLM[vLLM Instances]
        EXT[External APIs]
    end

    APP --> INF
    APP --> AGT
    APP --> RAG
    APP --> SAF
    APP --> MEM
    APP --> TOL

    INF --> PROV
    AGT --> PROV
    RAG --> VECT
    SAF --> SAFE
    MEM --> VECT
    TOL --> TOOL

    PROV --> OCP
    PROV --> VLLM
    VECT --> OCP
    SAFE --> OCP
    TOOL --> OCP
    TOOL --> EXT

    style Application fill:#e1f5ff
    style API fill:#fff4e1
    style Distribution fill:#e8f5e9
    style Infra fill:#f3e5f5
----

=== Why use Llama Stack with Red Hat AI?

Llama Stack on Red Hat OpenShift provides several key advantages for enterprise AI deployments:

**Standardization and portability**::
The unified API allows you to switch between different model providers (vLLM, AWS Bedrock, Azure OpenAI) without rewriting application code. This helps to address vendor lock-in across hybrid cloud deployments.

**Enterprise-grade operations**::
Running on OpenShift provides automated deployment, scaling, monitoring, and security features. The Llama Stack operator manages the lifecycle of your AI infrastructure using Kubernetes-native patterns.

**Production-ready tooling**::
Llama Stack includes built-in capabilities for production AI applications:
- **Agent framework**: Multi-turn conversations with tool calling and reasoning
- **RAG support**: Vector storage and retrieval for knowledge-grounded responses
- **Safety guardrails**: Content moderation and prompt injection protection
- **Observability**: OpenAPI-compliant endpoints with health checks and monitoring

**Model flexibility**::
Support for multiple model hosting approaches in a single distribution:
- **Local inference**: Using vLLM for low-latency, cost-effective serving
- **Model-as-a-Service**: Integration with cloud providers like AWS Bedrock
- **Mixed deployments**: Combine local and remote models based on workload requirements

**Developer experience**::
The OpenAPI specification and client SDKs in Python, Node.js, and other languages provide a consistent developer experience. The same code works across development, staging, and production environments.

.Llama Stack on Red Hat OpenShift deployment architecture
[mermaid,width=100%]
----
graph TB
    subgraph OCP["OpenShift Cluster"]
        subgraph NS["User Namespace (Project)"]
            subgraph CRD["LlamaStackDistribution CRD"]
                LLS[Llama Stack Server Pod<br/>Port: 8321<br/>Distribution: starter<br/>Storage: PVC 20Gi]
                VLLM_SVC[vLLM Service<br/>Model-as-a-Service<br/>Qwen3-14B-Gaudi]

                subgraph AGENT["Agent Application"]
                    FRONT[Frontend Pod<br/>Chat UI - Node.js]
                    BACK[Backend Pod<br/>Python Agent]
                end
            end

            ROUTE[Routes/Ingress<br/>chat-ui.apps.cluster.example.com]
        end

        subgraph RHAI["Red Hat OpenShift AI 3"]
            SERVE[Model Serving<br/>vLLM]
            GPU[GPU Acceleration<br/>Intel Gaudi]
            MLOPS[MLOps Pipelines]
        end
    end

    FRONT --> BACK
    BACK --> LLS
    LLS --> VLLM_SVC
    VLLM_SVC --> SERVE
    SERVE --> GPU
    ROUTE --> FRONT

    style OCP fill:#e3f2fd
    style NS fill:#fff3e0
    style CRD fill:#e8f5e9
    style AGENT fill:#fce4ec
    style RHAI fill:#f3e5f5
----

=== Llama Stack distributions

A distribution defines which providers and capabilities are available in your Llama Stack deployment. The `starter` distribution includes:

* **Inference**: vLLM for local model serving, with optional cloud provider support
* **Vector databases**: For RAG and semantic search
* **Tool runtime**: Built-in tools for web search (Tavily) and RAG
* **Safety**: Content moderation and guardrails
* **Agent orchestration**: Multi-turn conversations with tool calling

This lab uses the `starter` distribution with vLLM as the primary inference provider, configured to use Intel Gaudi accelerators through OpenShift AI's Model-as-a-Service (MaaS) capability.

== Deploying your Llama Stack instance

Your cluster and namespace have a Custom Resource Definition (CRD) available for the Kind *LlamaStackDistribution* therefore you can create your own instance of a Llama Stack Server. 

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
 oc api-resources | awk 'NR==1 || $1 ~ /llama/ {print $5 "\t" $4}' | column -t
----

[.console-output]
[source,bash,subs=attributes+]
----
KIND                    NAMESPACED
LlamaStackOperator      
LlamaStackDistribution  true
----

First git clone a copy of the project files

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
git clone https://github.com/burrsutter/fantaco-redhat-one-2026
cd fantaco-redhat-one-2026/
----

Red Hat OpenShift AI 3 comes with a supported version of Llama Stack Operator and a supported Llama Stack distribution named `rh-dev`.

The LlamaStack setup is configurable using a ConfigMap and the Llama StackDistribution CR as follows: 


[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
oc apply -f llama-stack-scripts/llamastack-configmap.yaml
----

[.console-output]
[source,bash,subs=attributes+]
----
configmap/llama-stack-config created
----

You can review here the setup we use for LiteMaaS as a vLLM endpoint:

[.console-input]
[source,yaml,role="copypaste",subs=attributes+]
----
oc get configmap llama-stack-config -o yaml
----

You can review the models available on the LiteMaaS using the following command:

[.console-input]
[source,yaml,role="copypaste",subs=attributes+]
----
curl -sS {litellm_api_base_url}/models   -H "Authorization: Bearer {litellm_virtual_key}" | jq
----


Review the Llama Stack Server manifest below and apply by copy and paste the command in the terminal in order to install a Llama Stack Distribution for vLLM using Model-as-a-Service (LiteMaaS):

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
oc apply -f - <<'EOF'
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  annotations:
    openshift.io/display-name: llamastack-distribution-vllm
  name: llamastack-distribution-vllm
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  replicas: 1
  server:
    containerSpec:
      command:
        - /bin/sh
        - '-c'
        - llama stack run /etc/llama-stack/run.yaml
      env:
        - name: VLLM_TLS_VERIFY
          value: 'false'
        - name: MILVUS_DB_PATH
          value: ~/.llama/milvus.db
        - name: FMS_ORCHESTRATOR_URL
          value: 'http://localhost'
        - name: VLLM_MAX_TOKENS
          value: '4096'
        - name: VLLM_API_URL
          value: '{litellm_api_base_url}'
        - name: VLLM_API_TOKEN
          value: '{litellm_virtual_key}'
        - name: LLAMA_STACK_CONFIG_DIR
          value: /opt/app-root/src/.llama/distributions/rh/
      name: llama-stack
      port: 8321
      resources:
        limits:
          cpu: '2'
          memory: 12Gi
        requests:
          cpu: 250m
          memory: 500Mi
    distribution:
      name: rh-dev
    userConfig:
      configMapName: llama-stack-config


EOF
----

[.console-output]
[source,bash,subs=attributes+]
----
llamastackdistribution.llamastack.io/llamastack-distribution-vllm created
----

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
oc get pods 
----

[.console-output]
[source,bash,subs=attributes+]
----
NAME                                            READY   STATUS    RESTARTS   AGE
llamastack-distribution-vllm-77897d9f8f-bzgbg   1/1     Running   0          4m22s
showroom-dd6dbbc4d-lq89g                        3/3     Running   0          17h
----

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
oc get services
----

[.console-output]
[source,bash,subs=attributes+]
----
NAME                                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
llamastack-distribution-vllm-service   ClusterIP   172.231.129.47   <none>        8321/TCP   5m18s
showroom                               ClusterIP   172.231.54.134   <none>        8080/TCP   17h
----

If you see the pod and service then congraluations, you have a successfully deployed Llama Stack.  In the next chapter, you will be exploring Llama Stack. 